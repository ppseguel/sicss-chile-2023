{"cells":[{"cell_type":"markdown","source":["# Recuerde hace una copia de este notebook en `File -> Save a copy in Drive`."],"metadata":{"id":"Zl4TFFqR-l9T"}},{"cell_type":"markdown","metadata":{"id":"Lg6QjgGuXsTh"},"source":["# Clustering"]},{"cell_type":"markdown","source":["Utilizaremos un dataset de precios de automóviles: https://www.kaggle.com/code/goyalshalini93/car-price-prediction-linear-regression-rfe/data"],"metadata":{"id":"NXygOjTqoCzG"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"YH3jdK_JkoO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"https://users.dcc.uchile.cl/~hsarmien/diplomado/dataset/CarPrice_Assignment.csv\",\n","                         sep = \",\",\n","                         encoding = \"utf8\")"],"metadata":{"id":"cd5Q2Gdhknrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(2)"],"metadata":{"id":"4KLp0QKNlGWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.dtypes"],"metadata":{"id":"nR6QmpZbfZbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr = df[['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase',\n","                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower',\n","                    'citympg', 'carlength','carwidth']]\n","dfr.head()"],"metadata":{"id":"k6pk4FklfFi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dummies(x,df):\n","    temp = pd.get_dummies(df[x], drop_first = True)\n","    df = pd.concat([df, temp], axis = 1)\n","    df.drop([x], axis = 1, inplace = True)\n","    return df"],"metadata":{"id":"S0GPhGqaiftn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr = dummies('fueltype',dfr)\n","dfr = dummies('aspiration',dfr)\n","dfr = dummies('carbody',dfr)\n","dfr = dummies('drivewheel',dfr)\n","dfr = dummies('enginetype',dfr)\n","dfr = dummies('cylindernumber',dfr)"],"metadata":{"id":"Zgg3Uiprigx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr.head(5)"],"metadata":{"id":"1f5QwDa6imt-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr.shape"],"metadata":{"id":"ASRn-qCzW2aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from sklearn.preprocessing import LabelEncoder\n","#label_encoder = LabelEncoder()\n","#dfr.loc[:,\"fueltype\"] = label_encoder.fit_transform(dfr['fueltype'])"],"metadata":{"id":"_fI2oEJ_j3zV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## K-Means\n","\n","K-means es un método simple para particionar datos en distintos clusters, que intenta minimizar la distancia de cada punto de un cluster a su centroide.\n","Para ejemplificar, y conocer cómo usarlo en `scikit-learn`, haremos un ejemplo práctico donde se ven claramente 3 clusters:"],"metadata":{"id":"3DAYzuR0C3E3"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n"],"metadata":{"id":"mB2JiqPFC5Oi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ejecutamos k-means y le indicamos que queremos que divida los datos en 3 clusters:"],"metadata":{"id":"VJ3GzWzpJOZl"}},{"cell_type":"code","source":["random_state = 20\n","kmeans = KMeans(n_clusters=3, n_init=20, max_iter=300, random_state=random_state)\n","kmeans.fit(dfr) # fit retorna a self\n","y_pred = kmeans.predict(dfr)"],"metadata":{"id":"MBhUJ8GxJNab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr.head(3)"],"metadata":{"id":"rdWJ8JckQCUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred"],"metadata":{"id":"ZAp8J5u2QG0D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lo que estamos haciendo es crear un objeto KMeans, que está configurado para asignar 3 clusters, y le damos un `random_state` para tener resultados replicables. `n_init` significa que vamos a correr el método `n_init` veces, donde cada vez que se inicie el método se generan centroides que parten de manera aleatoria, finalmente se queda con el resultado que tiene el error más bajo. `max_iter` nos dice el número máximo de iteraciones que hará el modelo en el caso de que no encuentre convergencia antes.\n","\n","Luego pasamos los datos al modelo para que corra el algorimo de clustering. Finalmente le pedimos que nos de los clusters asignados a cada valor de entrada."],"metadata":{"id":"Z6UzWmx0J0x3"}},{"cell_type":"markdown","source":["Con el objeto `kmeans` entrenado podemos preguntar por algunos de los resultados. Podemos revisar cuáles son los centroides finales del modelo con `cluster_centers_`, cuáles son las asignaciones por cada uno de los ejemplos con `labels_` (en este caso es lo mismo que `y_pred` que tenemos arriba), el error de los clusters con `inertia_` y cuántas iteraciones tomó encontrar este resultado con `n_iter_`."],"metadata":{"id":"iSOJZXyILNHe"}},{"cell_type":"code","source":["# aqui tenemos los centroides del modelo elegido\n","kmeans.cluster_centers_"],"metadata":{"id":"zlPq2dqzLN4W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["También podemos obtenes medias de, por ejemplo, cuántos datos quedaron en cada cluster. No hay nada automático para obtener ese resultado, pero podemos usar `numpy` para contar los clusters."],"metadata":{"id":"sMRcEhugLaLK"}},{"cell_type":"code","source":["counts = np.bincount(y_pred)\n","print(counts)\n","\n","# a veces tiene sentido filtrar los que tienen un valor mayor o igual a 0, ya que normalmente\n","# se asigna -1 cuando el dato es considerado ruido\n","# counts = np.bincount(y_pred[y_pred>=0])"],"metadata":{"id":"mfq53PsHLZVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por otro lado, las asignaciones por cluster se pueden incorporar a una nueva columna en el dataset (representando el *cluster*). **Sin embargo, hay que tener cuidado al agregar esta columna al dataframe original, pues (erróneamente) podrían usarla a futuro en las variables para estimar otro modelo de clustering.**"],"metadata":{"id":"_40Ah0qNLi8z"}},{"cell_type":"code","source":["dfr.shape"],"metadata":{"id":"EDA_2pvEdjcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_X = dfr.assign(cluster=kmeans.labels_)\n","new_X.head(10)"],"metadata":{"id":"UcPrAsryd1oN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_X[new_X['cluster'] == 1].describe()"],"metadata":{"id":"lZwvDkU2gUk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_X[new_X['cluster'] == 2].describe()"],"metadata":{"id":"vwWic3s_hsCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizando clusters con reducción de dimensionalidad"],"metadata":{"id":"6DUP_pM4gv5O"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","reduX = PCA(n_components=2, random_state=0).fit_transform(dfr)\n","\n","#print(reduX)"],"metadata":{"id":"1gZcNV68gu7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(reduX[:, 0], reduX[:, 1], c=kmeans.labels_)\n","plt.show()"],"metadata":{"id":"Y4167L1jS5KO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans.inertia_"],"metadata":{"id":"7hmLvNlLTgUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Estimando la cantidad de clusters\n","\n","En el ejemplo, creamos los datos nosotros, por lo que sabemos la cantidad de clusters que necesitamos. Sin embargo, esto no siempre es el caso, y como K-Means necesita este valor al momento de correr el algoritmo, no podemos dejarlo al azar.\n","\n","Una forma de estimar el número de clusters es mediante la suma de la diferencia al cuadrado entre los puntos de cada cluster (SSE). En `scikit-learn` este dato se llama `inertia_`. Una tecnica para encontrar un número razonable de clusters a usar es el método del codo, donde calculamos el SSE para varios números de clusters y graficamos como varia el SSE, eligiendo el \"mejor\". Este concepto de \"mejor\" no es claro, pero la idea es elegir el último cluster antes de encontrarnos con el punto de _diminishing returns_, que sería cuando aumentar a más clusters nos da una mejora muy pequeña respecto a la que estamos considerando actualmente.\n","\n","Veamos un ejemplo. Ejecutemos K-Means entre 1 y 15 clusters y grafiquemos cómo cambia el error a medida que aumentamos el número de clusters."],"metadata":{"id":"iHejfd64DDhM"}},{"cell_type":"code","source":["sse = []\n","\n","clusters = list(range(2, 16)) #range(1,41)\n","for k in clusters:\n","    kmeans = KMeans(n_clusters=k).fit(dfr)\n","    sse.append(kmeans.inertia_)\n","    #print(\"Kmeans silhouette para k = \", k, silhouette_score(dfr, kmeans.labels_))\n","\n","    #plot_silhouette(dfr, kmeans)\n","\n","plt.plot(clusters, sse, marker=\"o\")\n","plt.title(\"Metodo del codo de 1 a 15 clusters\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"vyWVOBctDC_1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El gráfico nos muestra el error de K-Means usando diferentes números de clusters.\n","Acá se puede notar que un valor óptimo es 3 (mirar donde se forma el `codo` o el punto tras el cual el error decrece de manera menos significativa).\n","Si eligiéramos 4 o más, veríamos más particiones, pero posiblemente estaríamos separando clusters ya existentes en clusters más pequeños.\n","Ojo que este método es una heurística y no siempre el `codo` es claramente visible."],"metadata":{"id":"db3-dpcpPJvO"}},{"cell_type":"markdown","source":["## Evaluación de clusters\n","\n","Evaluar la calidad de nuestros clusters es algo no trivial. En 2 dimensiones podemos claramente encontrar separaciones de clusters visualmente, asumiendo que es tan simple como mirar un gráfico. El tema es que rara vez tendremos solo 2 dimensiones en nuestros datos, y cuando ya tenemos más de 3 dimensiones, no podemos graficarlo de una forma directa y tenemos que depender de técnicas de reducción de dimensionalidad. Se verá más adelante en el curso, pero las salidas de estas técnicas no son realmente interpretables.\n","\n","En esta sección veremos algunas forma de evaluar clusters, sea visual o numéricamente.\n","\n","\n","### Matriz de similitud (proximidad)\n","\n","Uno de los métodos vistos en clases para evaluar la calidad de los clusters es haciendo una matriz de similitud. Estas matrices nos permiten ver qué tan cerca están los puntos pertenecientes a un cluster entre sí, y simultaneamente ver qué tan lejos están los puntos de un cluster de los otros clusters."],"metadata":{"id":"KNrFDarLPV4r"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import euclidean_distances\n","\n","def sim_matrix(features, labels):  #labels: a cual cluster pertenece cada dato [0,1,2,1,0]\n","    useful_labels = labels >= 0    #dbscan los datos outliers pertenecen al cluster -1\n","\n","    # primero ordenamos los datos en base al cluster que pertencen\n","    indices = np.argsort(labels[useful_labels])\n","    sorted_features = features.iloc[indices]\n","\n","    # calculamos las distancias entre todos los puntos\n","    d = euclidean_distances(sorted_features, sorted_features)\n","    return d\n","\n","def plot_sim(data, model):\n","    fig, ax = plt.subplots()\n","    dist = sim_matrix(data, model.labels_)\n","    im = ax.imshow(dist, cmap=\"jet\")\n","    fig.colorbar(im, ax=ax)"],"metadata":{"id":"HbR0XdsMPKNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans1 = KMeans(n_clusters=3, random_state=random_state).fit(dfr)\n"],"metadata":{"id":"LaTThv9TSwAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_sim(dfr, kmeans1)"],"metadata":{"id":"R1rLl3KaaqJu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Silhouette\n","\n","Presentaremos otra forma de evaluar clusters, esta vez de una manera no visual usando el coeficiente de Silhouette. Como se vio en clases, este coeficiente calcula para cada punto:\n","\n","1) su distancia promedio al resto de los puntos en su mismo clases, digamos `a`. En ingles esto se llama `mean intra-cluster distance`.\n","\n","2) su distancia promedio a todos los puntos del cluster mas cercano, digamos `b`. En ingles esto se llama `mean nearest-cluster distance`.\n","Entonces el coeficiente de Silhouette se calcula con la siguiente formula:\n","$$\\frac{b - a}{max(a, b)}$$\n","\n","Esta métrica esta en un rango entre -1 y 1, donde 1 significa que algo está bien asignado, -1 significa que algo está mal asignado porque hay otro cluster más similar, y 0 significa que hay solapamiento de clusters."],"metadata":{"id":"Ny3XYmrDbjPe"}},{"cell_type":"code","source":["from sklearn.metrics import silhouette_score"],"metadata":{"id":"Dpmv2D3MbiNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El coeficiente de Silhouette se calcula pasando el dataset y los labels asignados por el metodo de cluster:\n","```python\n","silhouette_score(<dataset>, <labels>)\n","```"],"metadata":{"id":"zKCVeTSTbpsl"}},{"cell_type":"markdown","source":["Calculemos el Silhouette score de los modelos entrenados en la parte anterior."],"metadata":{"id":"IYrRPtNvbuKG"}},{"cell_type":"code","source":["print(\"Kmeans silhouette\", silhouette_score(dfr, kmeans1.labels_))\n"],"metadata":{"id":"XdtSRaAEbq2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Otra forma de ver Silhouette\n","\n","En la parte anterior usamos el coeficiente para todos los datos. También podemos considerar el coeficiente para cada dato individual y graficar eso. También podemos usar esta técnica como una alternativa para encontrar el número de clusters que queremos usar."],"metadata":{"id":"uZ-HPKoUb6kg"}},{"cell_type":"code","source":["from sklearn.metrics import silhouette_samples"],"metadata":{"id":"leFDfGRWb7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_silhouette(dataset, model):\n","    use_indices = model.labels_ >= 0\n","    use_labels = model.labels_[use_indices]\n","    use_data = dataset.iloc[use_indices]\n","\n","    n_clusters = len(np.unique(use_labels))\n","\n","\n","    fig, ax1 = plt.subplots()\n","\n","    silhouette_avg = silhouette_score(use_data, use_labels)\n","    print(f\"The average silhouette_score for {model.__class__.__name__} is : {silhouette_avg}\")\n","    sample_silhouette_values = silhouette_samples(use_data, use_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        ith_cluster_silhouette_values = sample_silhouette_values[use_labels == i]\n","        ith_cluster_silhouette_values.sort()\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                            0, ith_cluster_silhouette_values, alpha=0.7)\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","        y_lower = y_upper + 10\n","\n","    ax1.set_title(f\"{model.__class__.__name__}\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","    ax1.set_yticks([])\n","\n"],"metadata":{"id":"qEuo_Tv1cC_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_silhouette(dfr, kmeans1)"],"metadata":{"id":"cD7fgfJ8cHBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DBSCAN\n","\n","Algoritmo de clustering basado en densidad. Este método permite identificar clusters cuyos datos contienen mucho ruido, outliers y presentan una forma poco clara de separar en un plano. Pero tiene la debilidad de no funcionar bien cuando los clusters tienen densidades variables o tenemos una dimensionalidad muy alta.\n","\n","DBSCAN está implementado en `scikit-learn` y necesita de los parametros `eps` y `min_samples`. `eps` corresponde a la distancia dentro de la cual se consideran 2 puntos vecinos, `min_samples` corresponde al `minpts` visto en clases, que es el número de vecinos que tiene que tener un punto para ser considerado un punto _core_.**bold text**"],"metadata":{"id":"dAmtwXTzoysT"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","from sklearn import datasets"],"metadata":{"id":"JYSltRuqkPa2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`eps` es el parametro más importante de DBSCAN, por lo que tenemos que elegirlo con cuidado. En este caso podemos ver que dice que hay 7 \"clusters\", de los cuales 6 son clusters reales y el resto es considerado como ruido."],"metadata":{"id":"bJ7Rl3HDpxR3"}},{"cell_type":"code","source":["eps = 350   # 0.3\n","min_samples = 5\n","\n","dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(dfr)"],"metadata":{"id":"95cazH8wpAwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","reduX = PCA(n_components=2, random_state=0).fit_transform(dfr)\n","plt.scatter(reduX[:, 0], reduX[:, 1], c=dbscan.labels_) # [1,0,-1,-1]\n","plt.title(f\"DBSCAN: eps={eps}, min_samples={min_samples}\")\n","plt.show()"],"metadata":{"id":"VhzFhrXrpDka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of clusters in labels, ignoring noise if present.\n","n_clusters_ = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n","n_noise_ = list(dbscan.labels_).count(-1)\n","\n","print(\"Estimated number of clusters: %d\" % n_clusters_)\n","print(\"Estimated number of noise points: %d\" % n_noise_)"],"metadata":{"id":"aistYdqMpExn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dbscan.labels_)"],"metadata":{"id":"DHW67U0ypO5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_sim(dfr, dbscan)"],"metadata":{"id":"gCWaS-ATpoyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Estimando eps con método de la rodilla\n","\n","La idea de este procedimiento es calcular la distancia promedio de cada punto a sus `k` vecinos más cercanos los cuales son graficados en orden ascendente. El objetivo es determinar la _rodilla_, que corresponde al valor óptimo de `eps`."],"metadata":{"id":"KtHvT_0TqMVr"}},{"cell_type":"code","source":["from sklearn.neighbors import NearestNeighbors\n","import numpy as np\n","\n","nbrs = NearestNeighbors(n_neighbors=3).fit(dfr)\n","distances, indices = nbrs.kneighbors(dfr)\n","\n","distances = np.sort(distances, axis=0)\n","distances = distances[:,1]\n","plt.axhline(y=700, color='r', linestyle='--') #Ajuste el valor para y\n","plt.plot(distances)"],"metadata":{"id":"BJzHQE8iqGZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X2, y2 = datasets.make_circles(n_samples=1500, factor=.5,noise=.05)"],"metadata":{"id":"MXzZN_AIqQN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X2[:,0], X2[:,1])"],"metadata":{"id":"9vPOpEKPqSjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nosotros humanos vemos claramente que hay 2 grupos de datos. Sin embargo, K-Means no es capaz de separar esos 2 clusters, y los métodos aglomerativos necesitan de ayudas como mátrices de conectividad para lograrlo. Veamos como DBSCAN sí puede."],"metadata":{"id":"r5WP431Yq_lf"}},{"cell_type":"code","source":["eps = 0.2\n","min_samples = 5\n","\n","dbscan_circles = DBSCAN(eps=eps, min_samples=min_samples).fit(X2)\n","plt.scatter(X2[:,0], X2[:,1], c=dbscan_circles.labels_)\n","plt.title(f\"DBSCAN: eps={eps}, min_samples={min_samples}\")\n","plt.show()"],"metadata":{"id":"IpjfbrucqibI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aquí podemos ver otros ejemplos con otros datasets con que las otras técnicas de clustering tienen problemas."],"metadata":{"id":"s3W81NdjrGpf"}},{"cell_type":"code","source":["X3, y3 = datasets.make_moons(n_samples=1500, noise=.05)\n","plt.scatter(X3[:,0], X3[:,1])"],"metadata":{"id":"uZ04idvBrC4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eps = 0.2\n","min_samples = 5\n","\n","dbscan_moon = DBSCAN(eps=eps, min_samples=min_samples).fit(X3)\n","plt.scatter(X3[:,0], X3[:,1], c=dbscan_moon.labels_)\n","plt.title(f\"DBSCAN: eps={eps}, min_samples={min_samples}\")\n","plt.show()"],"metadata":{"id":"PMPmU-CZrJcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","_filter_label = dbscan.labels_ >= 0\n","print(\"Dataset X DBSCAN\\t\", silhouette_score(dfr[_filter_label], dbscan.labels_[_filter_label]))"],"metadata":{"id":"JJ0olJ3WrMQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_silhouette(dfr, dbscan)"],"metadata":{"id":"jaHw_1LtrP9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering Jerárquico Aglomerativo (Hierarchical clustering)\n","\n","Otra forma de hacer clustering es mediante Clustering Jerárquico Aglomerativo. En este método lo que hacemos es partir con que cada dato es un cluster independiente de los demás, y luego, mediante una matriz de distancias vamos uniendo datos, creando anidaciones de clusters. Continuamos hasta que quede solo 1 cluster muy grande.\n","\n","Generalmente estos métodos se grafican como un dendrograma, y usan la distancia euclidiana para calcular las matrices de distancias. Dicho esto, se pueden usar otras métricas de distancia para calcular la matriz de afinidad pero en la mayoría de los casos usamos la distancia euclidiana.\n","\n","Vamos a presentar 4 criterios para ir uniendo los clusters. Estos corresponden a `complete`, `average`, `single` y `ward`. Aquí una descripción rápida de los criterios:\n","* `complete`: considera la distancia máxima entre 2 clusters\n","* `average`: considera la distancia promedio entre 2 clusters\n","* `single`: considera la distancia mínima entre 2 clusters\n","* `ward`: minimiza la varianza entre los 2 clusters"],"metadata":{"id":"j9kO5-lFK-4-"}},{"cell_type":"markdown","source":["Para trabajar con clustering jerárquico podemos usar `scikit-learn` o `scipy`. `scikit-learn` lamentablemente no tiene una forma directa de graficar los dendrogramas, pero `scipy` sí, así que presentaremos ambas por si alguna vez las necesitan.\n","\n","En `scipy` existe todo un módulo dedicado a clustering jerárquico [scipy.cluster.hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html). En particular aquí usaremos `linkage` para generar las uniones de cada dato y cluster, y `dendrogram` para graficar el árbol.\n","\n","En `scikit-learn` tenemos [sklearn.cluster.AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) para computar los clusters y asignar los labels a cada dato."],"metadata":{"id":"6GtzHzRYLESH"}},{"cell_type":"code","source":["from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.cluster import AgglomerativeClustering"],"metadata":{"id":"kPzy5KXtrV8Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aquí computamos los árboles usando cada uno de los criterios:"],"metadata":{"id":"rBJEjvDXLMpB"}},{"cell_type":"code","source":["complete = linkage(dfr, method=\"complete\")\n","single = linkage(dfr, method=\"single\")\n","average = linkage(dfr, method=\"average\")\n","ward = linkage(dfr, method=\"ward\")"],"metadata":{"id":"WxahtcdKLNCr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y ahora podemos graficar los árboles para ver como se distribuyen las ramas del árbol."],"metadata":{"id":"xh1Gy667LSuD"}},{"cell_type":"code","source":["dendrogram(complete)\n","plt.title(\"Linkage: Complete\")\n","plt.show()"],"metadata":{"id":"Ed1Q7zexLTSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dendrogram(single)\n","plt.title(\"Linkage: Single\")\n","plt.show()"],"metadata":{"id":"vtpFkMoILXts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dendrogram(average)\n","plt.title(\"Linkage: Average\")\n","plt.show()"],"metadata":{"id":"GJGxHz4hLcoZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dendrogram(ward)\n","plt.title(\"Linkage: Ward\")\n","plt.show()"],"metadata":{"id":"yfX-CEzFLk_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualmente podemos cortar el arbol en distintos puntos para ver cómo se distribuyen los datos en las ramas. Luego manualente decidir viendo el dendrograma cuál sería una buena distancia para cortar el árbol.\n","\n","Por ejemplo, en el caso de `ward`, 60000 parece ser un buen número para cortar."],"metadata":{"id":"fers_OVsLg2g"}},{"cell_type":"code","source":["dendrogram(ward)\n","plt.title(\"Linkage: Ward\")\n","plt.axhline(y=60000, color='r', linestyle='--')\n","plt.show()"],"metadata":{"id":"ReC1qQweLhYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hasta ahora hemos solo graficado los árboles, pero no hemos etiquetado los datos. Ahora podemos usar `scikit-learn` con `AgglomerativeClustering`. Aquí tenemos varias opciones.\n","* Si sabemos cuantos clusters queremos (viendo el dendrograma), agregamos el parametro `n_clusters` y lo dejamos en cuántos clusters queremos.\n","* Si sabemos a la distancia que queremos cortar (tambien viendo el dendrograma), entonces usamos el parametro `distance_threshold`.\n","* En el caso de que no usemos `linkage` podemos hacer correr el algoritmo y que genere todo el arbol (dejando `n_clusters=None` y `distance_threshold=0`), luego calcular la matriz de relaciones a mano, graficarla usando el dendrograma, decidir dónde cortar y volver a unos de los 2 puntos anteriores."],"metadata":{"id":"ZJbenUomMQLh"}},{"cell_type":"markdown","source":["Corramos primero generando todo el árbol."],"metadata":{"id":"fzeQSFNEMWzb"}},{"cell_type":"code","source":["ward_all = AgglomerativeClustering(n_clusters=None, linkage=\"ward\", distance_threshold=0).fit(dfr)\n","print(ward_all.n_clusters_)"],"metadata":{"id":"7DvJsVY1MQvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como generamos el árbol entero tenemos tantos clusters como datos! Ahora, viendo el dendrograma anterior de `ward` decidimos que queremos 3 clusters. Entonces usamos lo siguiente:"],"metadata":{"id":"rdx1AC3UMeK6"}},{"cell_type":"code","source":["ward_3 = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit(dfr)\n","print(ward_3.n_clusters_)"],"metadata":{"id":"_U48UffuMdQy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y también podemos obtener la etiquetas."],"metadata":{"id":"nth-i6NWMlYJ"}},{"cell_type":"code","source":["ward_3.labels_"],"metadata":{"id":"pCxnMFK-Manf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","reduX = PCA(n_components=2, random_state=0).fit_transform(dfr)\n","plt.scatter(reduX[:, 0], reduX[:, 1], c=ward_3.labels_)\n","plt.show()"],"metadata":{"id":"VnHsWT7JM4CM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"private_outputs":true},"hide_input":false,"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"4ff442374353e7cd8d0b317b986b0b774c361b92b00d7ea43e4b1384f67d1c1b"}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}